{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6f7f2d-9fd2-4e44-a1db-28021edb93ce",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Beta Prototyping for data processing\n",
    "\n",
    "Notebook Overview\n",
    "This notebook is designed as a tool for our sentiment analysis project. Its purpose is to simplify the data processing pipeline by providing functions to retrieve and extract text from various sources—including PDFs, news articles (with pop-up removal), DOC/DOCX files, and plain text files. Each extraction function includes proper error handling and detailed explanations to ensure clarity during further development and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ef70ca-e4d4-4e39-9271-7c98a9ae4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd87c4df-6451-4b32-bcc6-9bf4ebf2f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DET', 'some', 415, True), ('NOUN', 'text', 8206900633647566924, True)]\n"
     ]
    }
   ],
   "source": [
    "#example of how we can use spacy as a tokenizer to help us save time without having to write the tokenizer ourselves\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"some text\")\n",
    "print([(token.pos_, token.text, token.dep, token.is_alpha) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038b90f-8e01-461a-abd8-717e45bf87d1",
   "metadata": {},
   "source": [
    "# The following question then arises: how would our data pipeline look like along with spaCy?\n",
    "\n",
    "Let's say it's of our interest to retrieve text from news articles. Once we retrieve the text, we need to feed it through the data pipeline and have a ready data to later feed it into the model.\n",
    "<ol>\n",
    "    <li>Firstly, we have a document, or any other type of file that contains the text. We extract it. For the news it would have to be through scraping. Otherwise, the main model should also work fine for the </li>\n",
    "    <li>Then after we've extracted the data from these data formats, we have to cleanse the data from noise, and normalise it.</li>\n",
    "    <li>After the developed methods to extract data from such files, we should have a way to store them (we have not decided yet whether it will be a simple txt file, json, or other). While the data is stored, we apply the pipeline with the spacy to tokenise the document.</li>\n",
    "    <li>After we've tokenised it, it's important to do stop words removal: filtering out common words (e.g., “the,” “and”) that might not contribute meaningful sentiment. This is important since the sentiment analysis will only rely on the important words.</li>\n",
    "    <li>The last step would then be lemmatising the words and retrieving the stems of the words as we have to standarize the words that will be used for then feature extraction and transformation.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b201083",
   "metadata": {},
   "source": [
    "# First Part: The Extraction:\n",
    "\n",
    "Let's say we want to scrape the article from the news and do the sentiment analysis regarding a certain topic. First, we want to extract the title of the article, with the main article. For general text it would have to be some document, the title is extracted along with the content itself and stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882400a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import PyPDF2\n",
    "\n",
    "def extract_txt(file):\n",
    "    \"\"\"\n",
    "    Extracts a text from a normal .txt file\n",
    "\n",
    "    Args:\n",
    "        file (str): The path to the .txt file.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_doc(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a .doc or .docx file using the textract library.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .doc or .docx file.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = textract.process(file_path)\n",
    "        return text.decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def extract_pdf(file_path):\n",
    "        \"\"\"\n",
    "        Extracts text from a .pdf file using the PyPDF2 library.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the .pdf file.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text = \"\"\n",
    "            with open(file_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfFileReader(file)\n",
    "                for page in reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text+=page_text+\"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {file_path}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f7dbeb",
   "metadata": {},
   "source": [
    "## PDF Text Extraction\n",
    "The function above extracts text from PDF files using the PyPDF2 library. It reads the file in binary mode, processes each page, and concatenates the extracted text into a single string.\n",
    "\n",
    "## DOCX Text Extraction\n",
    "The second function leverages the python-docx library to extract text from DOCX files. It reads all paragraphs and joins them into one string, making it ideal for further text processing.\n",
    "\n",
    "## Plain Text Extraction\n",
    "A simple function at the beginning that reads a .txt file and returns its content as a string. This is useful for cases where the data is already in text format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea52758",
   "metadata": {},
   "source": [
    "## News Article Scraping\n",
    "\n",
    "Using Selenium with a headless Chrome browser, this function loads a news article URL, removes common registration pop-ups (by targeting typical modal CSS selectors), and scrapes the main article text. Additionally, it extracts key metadata (such as title, publication date, and author) by parsing the page’s meta tags with BeautifulSoup. You may need to tweak the selectors based on the website’s structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd11b0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Google\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_article_text(url):\n",
    "    \"\"\"\n",
    "    Loads a news article URL using Selenium, attempts to remove\n",
    "    any registration pop-up, and returns the extracted article text.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL of the news article.\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted text of the article.\n",
    "    \"\"\"\n",
    "    # Set up headless Chrome\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load by waiting for an <article> element\n",
    "        # Adjust the tag or selector if the site uses a different structure\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"article\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error waiting for the article to load:\", e)\n",
    "    \n",
    "    # Attempt to remove common modal/pop-up elements.\n",
    "    # The selectors here are examples; you may need to tweak them for your target site.\n",
    "    modal_selectors = [\".modal\", \".popup\", \".overlay\", \"#registration-modal\"]\n",
    "    for selector in modal_selectors:\n",
    "        try:\n",
    "            modals = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            for modal in modals:\n",
    "                driver.execute_script(\"\"\"\n",
    "                    var element = arguments[0];\n",
    "                    element.parentNode.removeChild(element);\n",
    "                \"\"\", modal)\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing element with selector {selector}: {e}\")\n",
    "\n",
    "    # Extract text: try to get text from an <article> tag if available,\n",
    "    # otherwise fall back to the full body text.\n",
    "    article_text = \"\"\n",
    "    try:\n",
    "        article = driver.find_element(By.TAG_NAME, \"article\")\n",
    "        article_text = article.text\n",
    "    except Exception as e:\n",
    "        print(\"No <article> tag found; falling back to <body> text.\", e)\n",
    "        article_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "\n",
    "    driver.quit()\n",
    "    return article_text\n",
    "\n",
    "\n",
    "#Check to make sure chromedriver works\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # optional: run in headless mode\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.get(\"https://www.google.com\")\n",
    "print(\"Title:\", driver.title)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d65ff4",
   "metadata": {},
   "source": [
    "## Future Enhancements\n",
    "\n",
    "<p>Ok great, how can we improve this?</p>\n",
    "\n",
    "One promising improvement is to enhance our data pipeline by also processing and saving metadata. We can extract key metadata—such as the article name, publication date, and author—from the source (when available) and store it along with the main text. The complete data will be saved as a JSON file in a dedicated folder (e.g., processed_articles). This folder will act as a staging area for our data pipeline, allowing us to later analyze who wrote each piece, their views on different topics, and how the overall sentiment or consensus forms around these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61f721",
   "metadata": {},
   "source": [
    "## Unified Text Extraction Class\n",
    "Below is a Python class that unifies the extraction process. It determines if the input is a URL or a file path and calls the appropriate extraction method. In the case of news articles, it also attempts to extract metadata and then offers a method to save the data as JSON.\n",
    "\n",
    "Below is a rewritten version of the unified extraction class. In this version, each extraction method attempts to extract both the main text and any available metadata—even for file formats like PDFs, DOCX, and plain text files. You can later extend the metadata extraction for TXT files if you decide on a header format, but for now, TXT files will simply yield an empty metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6190549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to processed_articles/sample_article.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import textract\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium for URL (news article) extraction\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Library for PDF extraction\n",
    "import PyPDF2\n",
    "\n",
    "class TextExtractor:\n",
    "    \"\"\"\n",
    "    A unified text extraction tool for sentiment analysis data processing.\n",
    "    \n",
    "    This class supports extracting text and metadata from various sources:\n",
    "      - PDF files (using PyPDF2)\n",
    "      - DOC and DOCX files (using textract)\n",
    "      - Plain text files (.txt)\n",
    "      - News articles via URL (using Selenium & BeautifulSoup)\n",
    "    \n",
    "    For each source, the method returns a tuple: (text, metadata)\n",
    "    where `metadata` is a dictionary containing key properties like title, author, date, etc.\n",
    "    Data is later saved as JSON in a dedicated output directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dir=\"processed_articles\"):\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "    \n",
    "    def extract_from_pdf(self, file_path):\n",
    "        \"\"\"\n",
    "        Extracts text and metadata from a PDF file.\n",
    "        \n",
    "        Metadata is extracted from the PDF's document info if available (e.g., Title, Author).\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {}\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                # Extract text from each page\n",
    "                for page in reader.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                # Extract metadata from the PDF document\n",
    "                if hasattr(reader, \"metadata\") and reader.metadata:\n",
    "                    pdf_meta = reader.metadata\n",
    "                    for key, value in pdf_meta.items():\n",
    "                        if value:\n",
    "                            # Remove the leading slash (e.g., '/Title') and convert key to lower case\n",
    "                            cleaned_key = key.lstrip('/').lower()\n",
    "                            metadata[cleaned_key] = value\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from PDF: {e}\")\n",
    "        return text.strip(), metadata\n",
    "    \n",
    "    def extract_from_doc(self, file_path):\n",
    "        \"\"\"\n",
    "        Extracts text from DOC and DOCX files using textract.\n",
    "        \n",
    "        Metadata extraction is not supported by textract, so an empty dictionary is returned.\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {}\n",
    "        try:\n",
    "            # textract returns a bytestring so decode it to UTF-8.\n",
    "            text = textract.process(file_path).decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from DOC/DOCX: {e}\")\n",
    "        return text.strip(), metadata\n",
    "    \n",
    "    def extract_from_txt(self, file_path):\n",
    "        \"\"\"\n",
    "        Reads text from a plain text (.txt) file.\n",
    "        \n",
    "        Currently, no metadata is extracted from TXT files.\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        metadata = {}\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading TXT file: {e}\")\n",
    "        return text.strip(), metadata\n",
    "    \n",
    "    def scrape_article_text(self, url):\n",
    "        \"\"\"\n",
    "        Loads a news article URL using Selenium, removes potential pop-ups,\n",
    "        and extracts both the article text and key metadata.\n",
    "        \n",
    "        Metadata is extracted from meta tags (e.g., og:title, article:published_time, article:author)\n",
    "        and falls back to <title> if necessary.\n",
    "        \"\"\"\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        article_text = \"\"\n",
    "        metadata = {}\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            # Wait for content to load (preferably an <article> tag)\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"article\"))\n",
    "                )\n",
    "                article = driver.find_element(By.TAG_NAME, \"article\")\n",
    "                article_text = article.text\n",
    "            except Exception:\n",
    "                article_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            \n",
    "            # Use BeautifulSoup to parse the page and extract metadata\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            # Extract title using og:title or <title>\n",
    "            title_tag = soup.find(\"meta\", property=\"og:title\")\n",
    "            if title_tag and title_tag.get(\"content\"):\n",
    "                metadata[\"title\"] = title_tag[\"content\"]\n",
    "            elif soup.title and soup.title.string:\n",
    "                metadata[\"title\"] = soup.title.string.strip()\n",
    "            \n",
    "            # Extract publication date (if available)\n",
    "            date_tag = soup.find(\"meta\", property=\"article:published_time\")\n",
    "            if date_tag and date_tag.get(\"content\"):\n",
    "                metadata[\"date\"] = date_tag[\"content\"]\n",
    "            \n",
    "            # Extract author information (if available)\n",
    "            author_tag = soup.find(\"meta\", property=\"article:author\")\n",
    "            if author_tag and author_tag.get(\"content\"):\n",
    "                metadata[\"author\"] = author_tag[\"content\"]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping article: {e}\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        \n",
    "        return article_text.strip(), metadata\n",
    "    \n",
    "    def extract_text(self, source):\n",
    "        \"\"\"\n",
    "        Determines whether the source is a URL or a file path,\n",
    "        then uses the appropriate extraction method.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (text, metadata)\n",
    "        \"\"\"\n",
    "        # Check if source is a URL\n",
    "        if re.match(r'^https?://', source):\n",
    "            return self.scrape_article_text(source)\n",
    "        else:\n",
    "            ext = os.path.splitext(source)[1].lower()\n",
    "            if ext == \".pdf\":\n",
    "                return self.extract_from_pdf(source)\n",
    "            elif ext in [\".doc\", \".docx\"]:\n",
    "                return self.extract_from_doc(source)\n",
    "            elif ext == \".txt\":\n",
    "                return self.extract_from_txt(source)\n",
    "            else:\n",
    "                print(\"Unsupported file format.\")\n",
    "                return \"\", {}\n",
    "    \n",
    "    def save_as_json(self, data, filename):\n",
    "        \"\"\"\n",
    "        Saves the provided data (text and metadata) as a JSON file in the output directory.\n",
    "        \n",
    "        Args:\n",
    "            data (dict): A dictionary containing extracted text and metadata.\n",
    "            filename (str): The filename for the saved JSON.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.output_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Data successfully saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving JSON file: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == '__main__':\n",
    "    extractor = TextExtractor()\n",
    "    \n",
    "    # Example 1: Extract from a news article URL\n",
    "    source_url = \"https://www.bbc.com/news/articles/c4gdwgjkk1no\"\n",
    "    text, metadata = extractor.extract_text(source_url)\n",
    "    data = {\"text\": text, \"metadata\": metadata}\n",
    "    extractor.save_as_json(data, \"sample_article.json\")\n",
    "    \n",
    "    # Example 2: Extract from a local DOCX file (also supports .doc)\n",
    "    # source_file = \"path/to/sample.docx\"\n",
    "    # text, metadata = extractor.extract_text(source_file)\n",
    "    # data = {\"text\": text, \"metadata\": metadata}\n",
    "    # extractor.save_as_json(data, \"sample_docx.json\")\n",
    "    \n",
    "    # Example 3: Extract from a local PDF file\n",
    "    # source_file = \"path/to/sample.pdf\"\n",
    "    # text, metadata = extractor.extract_text(source_file)\n",
    "    # data = {\"text\": text, \"metadata\": metadata}\n",
    "    # extractor.save_as_json(data, \"sample_pdf.json\")\n",
    "    \n",
    "    # Example 4: Extract from a local TXT file\n",
    "    # source_file = \"path/to/sample.txt\"\n",
    "    # text, metadata = extractor.extract_text(source_file)\n",
    "    # data = {\"text\": text, \"metadata\": metadata}\n",
    "    # extractor.save_as_json(data, \"sample_txt.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e80bec",
   "metadata": {},
   "source": [
    "## and ALAS\n",
    "it works!🚀🚀🚀\n",
    "If you notice, we create an instance of the class, then after we get the source_url of an article, use the extractor method to extract text that then redirects to another helper method for a specific data format, it then generates us the data saves it as a json. For metadata in txt and docs are not supported so likely the name of the title and the article are inside, which is not a problem for now, as we will tackle it. \n",
    "\n",
    "This is the result of the test:\n",
    "<img src=\"dummy_files/figure1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed026368",
   "metadata": {},
   "source": [
    "# Second Part: Noise Cleansing and Normalisation\n",
    "In this section, we focus on cleaning the extracted data by removing noise (e.g., HTML tags, punctuation, stopwords, etc.) and normalizing the text using spaCy. The goal is to prepare the text for downstream sentiment analysis by standardizing it (through processes such as lemmatization and lowercasing) and stripping out unnecessary noise.\n",
    "\n",
    "Below, we create a sample JSON file containing some text and metadata. We then load the JSON, process the text to remove unwanted elements and normalize it using spaCy, and finally display the results. Later on, this cleaning process can be refactored into its own function or class for integration into a larger data processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363bc8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "This is a sample article! It includes various, punctuation and UPPERCASE letters. And some noise... <script>alert('noise');</script>\n",
      "\n",
      "Cleaned and Normalized Text:\n",
      "sample article include punctuation uppercase letter noise alert('noise\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ---------------------------\n",
    "# Create a sample JSON file\n",
    "# ---------------------------\n",
    "sample_json = {\n",
    "    \"text\": \"This is a sample article! It includes various, punctuation and UPPERCASE letters. And some noise... <script>alert('noise');</script>\",\n",
    "    \"metadata\": {\n",
    "        \"title\": \"Sample Article\",\n",
    "        \"author\": \"Author Name\",\n",
    "        \"date\": \"2023-01-01\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('sample_article.json', 'w') as f:\n",
    "    json.dump(sample_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "#load the json file\n",
    "with open('sample_article.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#initialise the spacy nlp model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = re.sub(r'<[^>]+>', '', data['text'])\n",
    "\n",
    "doc = nlp(text)\n",
    "cleaned_tokens = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct and not token.like_num:\n",
    "        lemma = token.lemma_.lower().strip()\n",
    "        #skip pronouns or empty tokens\n",
    "        if lemma and lemma != '-pron-':\n",
    "            cleaned_tokens.append(lemma)\n",
    "cleaned_text = ' '.join(cleaned_tokens)\n",
    "\n",
    "# Update the JSON data with the cleaned text\n",
    "data[\"cleaned_text\"] = cleaned_text\n",
    "\n",
    "# Display the original and cleaned text\n",
    "print(\"Original Text:\")\n",
    "print(data[\"text\"])\n",
    "print(\"\\nCleaned and Normalized Text:\")\n",
    "print(data[\"cleaned_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808426f",
   "metadata": {},
   "source": [
    "ok, great, but we can improve it with Beautiful Soup and more regular expression to get rid of the noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cccf13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "This is a sample article! It includes various, punctuation and UPPERCASE letters. And some noise... <script>alert('noise');</script>\n",
      "\n",
      "Cleaned and Normalized Text:\n",
      "sample article include punctuation uppercase letter noise\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "with open(\"sample_article.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sample_json, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# ---------------------------\n",
    "# Load the sample JSON file and clean the text\n",
    "# ---------------------------\n",
    "with open(\"sample_article.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "text = data['text']\n",
    "# Use BeautifulSoup to parse the text and remove unwanted tags\n",
    "soup = BeautifulSoup(text, \"html.parser\")\n",
    "for tag in soup([\"script\", \"style\"]):\n",
    "    tag.decompose()  # Remove script and style elements completely\n",
    "# Extract text using BeautifulSoup's get_text method\n",
    "text = soup.get_text(separator=\" \")\n",
    "# Remove extra whitespace and newlines\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "# Process the cleaned text with spaCy\n",
    "doc = nlp(text)\n",
    "cleaned_tokens = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct and not token.like_num:\n",
    "        lemma = token.lemma_.lower().strip()\n",
    "        if lemma and lemma != '-pron-':\n",
    "            cleaned_tokens.append(lemma)\n",
    "    \n",
    "cleaned_text = \" \".join(cleaned_tokens)\n",
    "\n",
    "data[\"cleaned_text\"] = cleaned_text\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(data[\"text\"])\n",
    "print(\"\\nCleaned and Normalized Text:\")\n",
    "print(data[\"cleaned_text\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3191985",
   "metadata": {},
   "source": [
    "# Much Better! 🚀🚀🚀 ✅"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
