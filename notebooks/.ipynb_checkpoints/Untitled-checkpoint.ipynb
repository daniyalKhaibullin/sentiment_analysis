{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f6f7f2d-9fd2-4e44-a1db-28021edb93ce",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Beta Prototyping for data processing\n",
    "\n",
    "The following notebook is used for exploration purposes. \n",
    "It aims to test and experiment with different data processing methods to determine which one is the best for the use of the sentiment analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ef70ca-e4d4-4e39-9271-7c98a9ae4a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd87c4df-6451-4b32-bcc6-9bf4ebf2f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DET', 'some', 415, True), ('NOUN', 'text', 8206900633647566924, True)]\n"
     ]
    }
   ],
   "source": [
    "#example of how we can use spacy as a tokenizer to help us save time without having to write the tokenizer ourselves\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"some text\")\n",
    "print([(token.pos_, token.text, token.dep, token.is_alpha) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038b90f-8e01-461a-abd8-717e45bf87d1",
   "metadata": {},
   "source": [
    "# The following question then arises: how would our data pipeline look like along with spaCy?\n",
    "<ol>\n",
    "    <li>Firstly, we have a document, or any other type of file that contains the text. These are our messages, our tweets, our collection of texts in text files, csv files, xml files, json files, and so on. These can also be text from unscraped html files, and so on.</li>\n",
    "    <li>Then after we've extracted the data from these data formats, we have to cleanse the data from noise, and normalise it.</li>\n",
    "    <li>After the developed methods to extract data from such files, we should have a way to store them (we have not decided yet whether it will be a simple txt file, json, or other). While the data is stored, we apply the pipeline with the spacy to tokenise the document.</li>\n",
    "    <li>After we've tokenised it, it's important to do stop words removal: filtering out common words (e.g., “the,” “and”) that might not contribute meaningful sentiment. This is important since the sentiment analysis will only rely on the important words.</li>\n",
    "    <li>The last step would then be lemmatising the words and retrieving the stems of the words as we have to standarize the words that will be used for then feature extraction and transformation.</li>\n",
    "</ol>\n",
    "\n",
    "This is how the pipeline would look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e28f2d-629a-4785-a398-0ffff75f3a9d",
   "metadata": {},
   "source": [
    "<img src=\"diagram.png\" alt=\"Overall image\" width=\"800\" length=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613b92a-752b-450b-a55c-113bf4d6d11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sent_env)",
   "language": "python",
   "name": "sent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
